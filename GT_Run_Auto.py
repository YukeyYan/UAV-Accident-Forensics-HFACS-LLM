#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GT_Run_Auto - HFACSåˆ†ç±»è‡ªåŠ¨åŒ–è¯„ä¼°å™¨
åŸºäºGT_Runçš„è¯„ä¼°é€»è¾‘ï¼Œä½¿ç”¨OpenAIå’Œæœ¬åœ°LLMè‡ªåŠ¨è¯„ä¼°HFACSåˆ†ç±»ç»“æœçš„æ­£ç¡®æ€§
æ”¯æŒ18ç±»åˆ«+4å±‚çº§åŒé‡è¯„ä¼°ï¼Œæ‰¹é‡å¤„ç†ï¼ŒCSVå¯¼å‡º
"""

import argparse
import json
import os
import sys
import glob
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
from openai import OpenAI
from tqdm import tqdm
import requests
import pandas as pd
import re  # âœ… ç”¨äºä»LLMè¿”å›æ–‡æœ¬ä¸­æå–JSON

# 18ä¸ªHFACS 8.0åˆ†ç±»å®šä¹‰
HFACS_CATEGORIES = [
    "UNSAFE ACTSâ€”Errorsâ€”Performance/Skill-Based",
    "UNSAFE ACTSâ€”Errorsâ€”Judgement & Decision-Making", 
    "UNSAFE ACTSâ€”Known Deviations",
    "PRECONDITIONSâ€”Physical Environment",
    "PRECONDITIONSâ€”Technological Environment",
    "PRECONDITIONSâ€”Team Coordination/Communication",
    "PRECONDITIONSâ€”Training Conditions",
    "PRECONDITIONSâ€”Mental Awareness (Attention)",
    "PRECONDITIONSâ€”State of Mind",
    "PRECONDITIONSâ€”Adverse Physiological",
    "SUPERVISION/LEADERSHIPâ€”Unit Safety Culture",
    "SUPERVISION/LEADERSHIPâ€”Supervisory Known Deviations",
    "SUPERVISION/LEADERSHIPâ€”Ineffective Supervision",
    "SUPERVISION/LEADERSHIPâ€”Ineffective Planning & Coordination",
    "ORGANIZATIONAL INFLUENCESâ€”Climate/Culture",
    "ORGANIZATIONAL INFLUENCESâ€”Policy/Procedures/Process",
    "ORGANIZATIONAL INFLUENCESâ€”Resource Support",
    "ORGANIZATIONAL INFLUENCESâ€”Training Program Issues"
]

# HFACSå››ä¸ªå±‚çº§
HFACS_LAYERS = [
    "UNSAFE ACTS", 
    "PRECONDITIONS", 
    "SUPERVISION/LEADERSHIP", 
    "ORGANIZATIONAL INFLUENCES"
]

# ç±»åˆ«åˆ°å±‚çº§çš„æ˜ å°„
CATEGORY_TO_LAYER = {}
for category in HFACS_CATEGORIES:
    for layer in HFACS_LAYERS:
        if category.startswith(layer):
            CATEGORY_TO_LAYER[category] = layer
            break

# è¯„ä¼°ä¸“ç”¨çš„Function Schema
EVALUATION_FUNCTION_SCHEMA = {
    "name": "evaluate_hfacs_classification",
    "description": "Evaluate the correctness of HFACS classifications for multiple sentences",
    "parameters": {
        "type": "object",
        "properties": {
            "evaluations": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "sentence_id": {"type": "string"},
                        "category_is_correct": {
                            "type": "integer",
                            "enum": [0, 1, -1],
                            "description": "Whether the HFACS category classification is correct: 1=correct, 0=incorrect, -1=skip/uncertain"
                        },
                        "category_evaluation_reason": {
                            "type": "string",
                            "description": "Brief reason for category evaluation decision"
                        },
                        "layer_is_correct": {
                            "type": "integer", 
                            "enum": [0, 1, -1],
                            "description": "Whether the HFACS layer classification is correct: 1=correct, 0=incorrect, -1=skip/uncertain"
                        },
                        "layer_evaluation_reason": {
                            "type": "string",
                            "description": "Brief reason for layer evaluation decision"
                        },
                        "final_category": {
                            "type": "string",
                            "description": "Final HFACS category (one of 18 categories). If category_is_correct=1, use original; if 0, select correct one; if -1, leave empty"
                        },
                        "final_layer": {
                            "type": "string",
                            "description": "Final HFACS layer (one of 4 layers). If layer_is_correct=1, use original; if 0, select correct one; if -1, leave empty"
                        }
                    },
                    "required": [
                        "sentence_id",
                        "category_is_correct", 
                        "category_evaluation_reason",
                        "layer_is_correct",
                        "layer_evaluation_reason",
                        "final_category",
                        "final_layer"
                    ]
                }
            }
        },
        "required": ["evaluations"]
    }
}

# ç³»ç»Ÿæç¤ºè¯ - ä¸“é—¨ç”¨äºè¯„ä¼°HFACSåˆ†ç±»æ­£ç¡®æ€§ï¼ˆå¼ºè°ƒä¿æŒ"æ€€ç–‘"æ€åº¦ï¼‰
EVALUATION_SYSTEM_PROMPT = """You are an expert aviation-safety analyst specialised in HFACS (Human Factors Analysis and Classification System) evaluation.

Your mission is to SKEPTICALLY evaluate whether each provided HFACS classification is correct AND provide the final correct classification.

Analyse for every sentence:
1. The raw sentence text.
2. The assigned HFACS category (one of the 18 specific categories).
3. The assigned HFACS layer (one of the 4 general layers).
4. The original reasoning that was given for the classification.

When judging, keep a critical mindset â€“ if evidence is weak or ambiguous, mark the classification incorrect (0) or uncertain (-1). Do NOT give credit without strong textual support.

EVALUATION CRITERIA:
â€¢ Category: Is the 18-category classification accurate?
â€¢ Layer: Is the 4-layer classification accurate?

EVALUATION VALUES:
â€¢ 1 = Correct
â€¢ 0 = Incorrect
â€¢ -1 = Skip / Uncertain (evidence ambiguous)

FINAL CLASSIFICATION RULES:
â€¢ final_category: 
  - If category_is_correct = 1, use the original assigned category
  - If category_is_correct = 0, select the correct category from the 18 HFACS categories
  - If category_is_correct = -1, leave empty string ""
â€¢ final_layer:
  - If layer_is_correct = 1, use the original assigned layer
  - If layer_is_correct = 0, select the correct layer from the 4 HFACS layers
  - If layer_is_correct = -1, leave empty string ""

The 18 HFACS categories are:
1. UNSAFE ACTSâ€”Errorsâ€”Performance/Skill-Based
2. UNSAFE ACTSâ€”Errorsâ€”Judgement & Decision-Making
3. UNSAFE ACTSâ€”Known Deviations
4. PRECONDITIONSâ€”Physical Environment
5. PRECONDITIONSâ€”Technological Environment
6. PRECONDITIONSâ€”Team Coordination/Communication
7. PRECONDITIONSâ€”Training Conditions
8. PRECONDITIONSâ€”Mental Awareness (Attention)
9. PRECONDITIONSâ€”State of Mind
10. PRECONDITIONSâ€”Adverse Physiological
11. SUPERVISION/LEADERSHIPâ€”Unit Safety Culture
12. SUPERVISION/LEADERSHIPâ€”Supervisory Known Deviations
13. SUPERVISION/LEADERSHIPâ€”Ineffective Supervision
14. SUPERVISION/LEADERSHIPâ€”Ineffective Planning & Coordination
15. ORGANIZATIONAL INFLUENCESâ€”Climate/Culture
16. ORGANIZATIONAL INFLUENCESâ€”Policy/Procedures/Process
17. ORGANIZATIONAL INFLUENCESâ€”Resource Support
18. ORGANIZATIONAL INFLUENCESâ€”Training Program Issues

The 4 HFACS layers are:
1. UNSAFE ACTS
2. PRECONDITIONS
3. SUPERVISION/LEADERSHIP
4. ORGANIZATIONAL INFLUENCES

REASONING GUIDELINES:
â€¢ Be concise (â‰¤ 30 words per reason)
â€¢ Explicitly reference sentence content when possible
â€¢ Remain objective and consistent with HFACS definitions
â€¢ Default to doubt; better to mark 0/-1 than to approve a wrong label"""

# Few-shotç¤ºä¾‹ï¼šè¦†ç›–å…¨éƒ¨18ç±»ï¼Œæ¯ç±» 1 å¥ã€‚å‰ 10 æ¡ç¤ºèŒƒ"æ­£ç¡®"è¯„ä¼°ï¼Œå 8 æ¡ç¤ºèŒƒ"é”™è¯¯"è¯„ä¼°ï¼Œä½“ç°æ‰¹åˆ¤æ€ç»´ã€‚
_FEW_SHOT_USER_CONTENT = """Evaluate these HFACS classifications:

ID: s1
Text: "The mechanic torqued the main-rotor nut 40 % beyond the published limit."
Assigned Category: UNSAFE ACTSâ€”Errorsâ€”Performance/Skill-Based
Assigned Layer: UNSAFE ACTS
Original Reason: "Over-torque due to poor skill usage."

ID: s2
Text: "The pilot elected VFR into forecast embedded thunderstorms despite an IFR alternative."
Assigned Category: UNSAFE ACTSâ€”Errorsâ€”Judgement & Decision-Making
Assigned Layer: UNSAFE ACTS
Original Reason: "Risky decision-making shows judgement error."

ID: s3
Text: "Crew knowingly disabled the landing-gear warning horn to eliminate its noise."
Assigned Category: UNSAFE ACTSâ€”Known Deviations
Assigned Layer: UNSAFE ACTS
Original Reason: "Deliberate violation of safety device."

ID: s4
Text: "Freezing rain coated the runway with ice during landing."
Assigned Category: PRECONDITIONSâ€”Physical Environment
Assigned Layer: PRECONDITIONS
Original Reason: "Adverse weather is physical environment factor."

ID: s5
Text: "A false stall warning distracted the crew during climb-out."
Assigned Category: PRECONDITIONSâ€”Technological Environment
Assigned Layer: PRECONDITIONS
Original Reason: "Malfunctioning warning system is technological factor."

ID: s6
Text: "Captain ignored the first-officer's 'minimums' call-out and continued descent."
Assigned Category: PRECONDITIONSâ€”Team Coordination/Communication
Assigned Layer: PRECONDITIONS
Original Reason: "Breakdown in crew communication."

ID: s7
Text: "The loader had never been trained on the new hydraulic hoist."
Assigned Category: PRECONDITIONSâ€”Training Conditions
Assigned Layer: PRECONDITIONS
Original Reason: "Lack of task-specific training."

ID: s8
Text: "Pilot fixated on the TCAS display and missed altitude deviations."
Assigned Category: PRECONDITIONSâ€”Mental Awareness (Attention)
Assigned Layer: PRECONDITIONS
Original Reason: "Attention fixation degraded situational awareness."

ID: s9
Text: "Engineer dismissed rising vibration as normal for the old airframe."
Assigned Category: PRECONDITIONSâ€”State of Mind
Assigned Layer: PRECONDITIONS
Original Reason: "Complacency influenced hazard appraisal."

ID: s10
Text: "Driver had been awake for 22 hours before starting the night convoy."
Assigned Category: PRECONDITIONSâ€”Adverse Physiological
Assigned Layer: PRECONDITIONS
Original Reason: "Severe fatigue impairs performance."

ID: s11
Text: "Squadron slogan 'mission first, paperwork later' discouraged near-miss reporting."
Assigned Category: SUPERVISION/LEADERSHIPâ€”Unit Safety Culture
Assigned Layer: SUPERVISION/LEADERSHIP
Original Reason: "Local culture prioritised schedule over safety."

ID: s12
Text: "Commander ordered crews to skip brake-temperature checks to save time."
Assigned Category: SUPERVISION/LEADERSHIPâ€”Supervisory Known Deviations
Assigned Layer: SUPERVISION/LEADERSHIP
Original Reason: "Supervisor knowingly waived mandatory step."

ID: s13
Text: "Scheduled maintenance audits were never conducted despite written policy."
Assigned Category: SUPERVISION/LEADERSHIPâ€”Ineffective Supervision
Assigned Layer: SUPERVISION/LEADERSHIP
Original Reason: "Lack of oversight allowed unsafe practice."

ID: s14
Text: "Duty roster paired two low-time pilots on a night cargo run."
Assigned Category: SUPERVISION/LEADERSHIPâ€”Ineffective Planning & Coordination
Assigned Layer: SUPERVISION/LEADERSHIP
Original Reason: "Poor crew pairing shows bad planning."

ID: s15
Text: "Headquarters campaign 'Fly more, spend less' pushed bases to lengthen duty days."
Assigned Category: ORGANIZATIONAL INFLUENCESâ€”Climate/Culture
Assigned Layer: ORGANIZATIONAL INFLUENCES
Original Reason: "High-level culture increased operational risk."

ID: s16
Text: "Two maintenance manuals listed conflicting torque limits for the same engine model."
Assigned Category: ORGANIZATIONAL INFLUENCESâ€”Policy/Procedures/Process
Assigned Layer: ORGANIZATIONAL INFLUENCES
Original Reason: "Unclear documentation is process flaw."

ID: s17
Text: "Funding shortages delayed replacement of cracked tow-bars across the fleet."
Assigned Category: ORGANIZATIONAL INFLUENCESâ€”Resource Support
Assigned Layer: ORGANIZATIONAL INFLUENCES
Original Reason: "Inadequate resources left hazards unmitigated."

ID: s18
Text: "The formal school syllabus for the new UAV lacked any night-operations module."
Assigned Category: ORGANIZATIONAL INFLUENCESâ€”Training Program Issues
Assigned Layer: ORGANIZATIONAL INFLUENCES
Original Reason: "Training program omitted essential content."
"""

# ç”Ÿæˆå¯¹åº”çš„ assistant function-callç»“æœï¼ˆå‰10æ­£ç¡®ï¼Œå8é”™è¯¯ï¼‰

# å®šä¹‰18ä¸ªç¤ºä¾‹çš„åŸå§‹åˆ†ç±»ï¼ˆä¸ä¸Šé¢çš„ç¤ºä¾‹å¯¹åº”ï¼‰
_EXAMPLE_CATEGORIES = [
    "UNSAFE ACTSâ€”Errorsâ€”Performance/Skill-Based",  # s1
    "UNSAFE ACTSâ€”Errorsâ€”Judgement & Decision-Making",  # s2
    "UNSAFE ACTSâ€”Known Deviations",  # s3
    "PRECONDITIONSâ€”Physical Environment",  # s4
    "PRECONDITIONSâ€”Technological Environment",  # s5
    "PRECONDITIONSâ€”Team Coordination/Communication",  # s6
    "PRECONDITIONSâ€”Training Conditions",  # s7
    "PRECONDITIONSâ€”Mental Awareness (Attention)",  # s8
    "PRECONDITIONSâ€”State of Mind",  # s9
    "PRECONDITIONSâ€”Adverse Physiological",  # s10
    "SUPERVISION/LEADERSHIPâ€”Unit Safety Culture",  # s11
    "SUPERVISION/LEADERSHIPâ€”Supervisory Known Deviations",  # s12
    "SUPERVISION/LEADERSHIPâ€”Ineffective Supervision",  # s13
    "SUPERVISION/LEADERSHIPâ€”Ineffective Planning & Coordination",  # s14
    "ORGANIZATIONAL INFLUENCESâ€”Climate/Culture",  # s15
    "ORGANIZATIONAL INFLUENCESâ€”Policy/Procedures/Process",  # s16
    "ORGANIZATIONAL INFLUENCESâ€”Resource Support",  # s17
    "ORGANIZATIONAL INFLUENCESâ€”Training Program Issues"  # s18
]

_EXAMPLE_LAYERS = [
    "UNSAFE ACTS",  # s1
    "UNSAFE ACTS",  # s2
    "UNSAFE ACTS",  # s3
    "PRECONDITIONS",  # s4
    "PRECONDITIONS",  # s5
    "PRECONDITIONS",  # s6
    "PRECONDITIONS",  # s7
    "PRECONDITIONS",  # s8
    "PRECONDITIONS",  # s9
    "PRECONDITIONS",  # s10
    "SUPERVISION/LEADERSHIP",  # s11
    "SUPERVISION/LEADERSHIP",  # s12
    "SUPERVISION/LEADERSHIP",  # s13
    "SUPERVISION/LEADERSHIP",  # s14
    "ORGANIZATIONAL INFLUENCES",  # s15
    "ORGANIZATIONAL INFLUENCES",  # s16
    "ORGANIZATIONAL INFLUENCES",  # s17
    "ORGANIZATIONAL INFLUENCES"  # s18
]

# ä¸ºå8ä¸ªé”™è¯¯ç¤ºä¾‹å®šä¹‰æ­£ç¡®çš„åˆ†ç±»
_CORRECT_CATEGORIES_FOR_ERRORS = [
    "SUPERVISION/LEADERSHIPâ€”Ineffective Supervision",  # s11 åº”è¯¥æ˜¯è¿™ä¸ª
    "UNSAFE ACTSâ€”Known Deviations",  # s12 åº”è¯¥æ˜¯è¿™ä¸ª
    "ORGANIZATIONAL INFLUENCESâ€”Policy/Procedures/Process",  # s13 åº”è¯¥æ˜¯è¿™ä¸ª
    "SUPERVISION/LEADERSHIPâ€”Ineffective Supervision",  # s14 åº”è¯¥æ˜¯è¿™ä¸ª
    "ORGANIZATIONAL INFLUENCESâ€”Policy/Procedures/Process",  # s15 åº”è¯¥æ˜¯è¿™ä¸ª
    "ORGANIZATIONAL INFLUENCESâ€”Training Program Issues",  # s16 åº”è¯¥æ˜¯è¿™ä¸ª
    "ORGANIZATIONAL INFLUENCESâ€”Policy/Procedures/Process",  # s17 åº”è¯¥æ˜¯è¿™ä¸ª
    "PRECONDITIONSâ€”Training Conditions"  # s18 åº”è¯¥æ˜¯è¿™ä¸ª
]

_CORRECT_LAYERS_FOR_ERRORS = [
    "SUPERVISION/LEADERSHIP",  # s11
    "UNSAFE ACTS",  # s12
    "ORGANIZATIONAL INFLUENCES",  # s13
    "SUPERVISION/LEADERSHIP",  # s14
    "ORGANIZATIONAL INFLUENCES",  # s15
    "ORGANIZATIONAL INFLUENCES",  # s16
    "ORGANIZATIONAL INFLUENCES",  # s17
    "PRECONDITIONS"  # s18
]

_ASSIST_EVALS = []
for i in range(1, 19):
    sid = f"s{i}"
    correct = 1 if i <= 10 else 0  # å‰10æ­£ç¡®ï¼Œå8é”™è¯¯
    layer_correct = 1 if i <= 10 else 0
    
    if correct:
        cat_reason = "Sentence content aligns with assigned category"
        layer_reason = "Layer correctly matches the category"
        final_category = _EXAMPLE_CATEGORIES[i-1]
        final_layer = _EXAMPLE_LAYERS[i-1]
    else:
        cat_reason = "Sentence content does not support assigned category"
        layer_reason = "Layer assignment is incorrect"
        # å¯¹äºé”™è¯¯çš„ç¤ºä¾‹ï¼Œä½¿ç”¨æ­£ç¡®çš„åˆ†ç±»
        final_category = _CORRECT_CATEGORIES_FOR_ERRORS[i-11]
        final_layer = _CORRECT_LAYERS_FOR_ERRORS[i-11]
        
    _ASSIST_EVALS.append({
        "sentence_id": sid,
        "category_is_correct": correct,
        "category_evaluation_reason": cat_reason,
        "layer_is_correct": layer_correct,
        "layer_evaluation_reason": layer_reason,
        "final_category": final_category,
        "final_layer": final_layer
    })

# ç»„è£…few-shotæ¶ˆæ¯åˆ—è¡¨
EVALUATION_FEW_SHOT = [
    {"role": "user", "content": _FEW_SHOT_USER_CONTENT},
    {
        "role": "assistant",
        "content": None,
        "function_call": {
            "name": "evaluate_hfacs_classification",
            "arguments": json.dumps({"evaluations": _ASSIST_EVALS})
        }
    }
]

class OllamaClient:
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
        
    def chat_completion(self, model: str, messages: list, temperature: float = 0.1,
                        max_tokens: int = 4000, force_json: bool = True):
        """è°ƒç”¨Ollama APIè¿›è¡ŒèŠå¤©è¡¥å…¨"""
        url = f"{self.base_url}/api/chat"
        
        data = {
            "model": model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
                # âœ… Ollama å®˜æ–¹æ”¯æŒ format=jsonï¼Œç”¨äºç¦æ­¢é¢å¤–è¯´æ˜æ–‡å­—
                **({"format": "json"} if force_json else {})
            }
        }
        
        try:
            response = requests.post(url, json=data, timeout=300)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            raise Exception(f"Ollama APIè°ƒç”¨å¤±è´¥: {str(e)}")

def test_ollama_connection(ollama_client: OllamaClient, model: str) -> bool:
    """æµ‹è¯•Ollamaè¿æ¥"""
    try:
        response = ollama_client.chat_completion(
            model=model,
            messages=[{"role": "user", "content": "Hello"}],
            temperature=0.1,
            max_tokens=10
        )
        return response and 'message' in response
    except Exception:
        return False

def load_classification_results(json_file: str) -> Dict[str, List[Dict]]:
    """åŠ è½½åˆ†ç±»ç»“æœJSONæ–‡ä»¶"""
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        raise Exception(f"åŠ è½½åˆ†ç±»ç»“æœæ–‡ä»¶å¤±è´¥: {str(e)}")

def extract_classification_items(classification_data: Dict[str, List[Dict]]) -> List[Dict]:
    """ä»åˆ†ç±»ç»“æœä¸­æå–æ‰€æœ‰åˆ†ç±»æ¡ç›®"""
    items = []
    for category, predictions in classification_data.items():
        # å¤„ç†æ‰€æœ‰åˆ†ç±»ï¼Œé€šè¿‡å‰ç¼€åŒ¹é…æ¨æ–­å±‚çº§
        layer = "Unknown"
        for hfacs_layer in HFACS_LAYERS:
            if category.startswith(hfacs_layer):
                layer = hfacs_layer
                break
        
        # å¦‚æœæ˜¯æ ‡å‡†åˆ†ç±»ï¼Œä½¿ç”¨æ˜ å°„è¡¨
        if category in CATEGORY_TO_LAYER:
            layer = CATEGORY_TO_LAYER[category]
            
        for pred in predictions:
            items.append({
                'sentence_id': f"s{pred.get('index', -1)}",
                'index': pred.get('index', -1),
                'text': pred.get('text', ''),
                'category': category,
                'layer': layer,
                'reason': pred.get('reason', '')
            })
    return items

def chunk(lst, size):
    """å°†åˆ—è¡¨åˆ†å—"""
    for i in range(0, len(lst), size):
        yield lst[i : i + size]

def evaluate_with_openai(items: List[Dict], batch_size: int, model: str, client: OpenAI, temperature: float = 0.1, enable_few_shot: bool = True) -> List[Dict]:
    """ä½¿ç”¨OpenAIè¿›è¡ŒHFACSåˆ†ç±»è¯„ä¼°"""
    evaluations = []
    
    for batch_idx, batch in enumerate(tqdm(list(chunk(items, batch_size)), desc="OpenAI-evaluating")):
        # æ„å»ºè¯„ä¼°å†…å®¹
        evaluation_content = "Evaluate these HFACS classifications:\n\n"
        for item in batch:
            evaluation_content += f"ID: {item['sentence_id']}\n"
            evaluation_content += f"Text: \"{item['text']}\"\n"
            evaluation_content += f"Assigned Category: {item['category']}\n"
            evaluation_content += f"Assigned Layer: {item['layer']}\n"
            evaluation_content += f"Original Reason: \"{item['reason']}\"\n\n"
        
        # æ„å»ºæ¶ˆæ¯
        messages = [{"role": "system", "content": EVALUATION_SYSTEM_PROMPT}]
        
        # æ ¹æ®å¼€å…³å†³å®šæ˜¯å¦æ·»åŠ few-shotç¤ºä¾‹
        if enable_few_shot:
            messages.extend(EVALUATION_FEW_SHOT)
        
        # æ·»åŠ å½“å‰è¯„ä¼°ä»»åŠ¡
        messages.append({"role": "user", "content": evaluation_content})
        
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=messages,
                functions=[EVALUATION_FUNCTION_SCHEMA],
                function_call={"name": "evaluate_hfacs_classification"},
                temperature=temperature
            )
            
            msg = resp.choices[0].message
            if msg.function_call:
                result = json.loads(msg.function_call.arguments)
                batch_evaluations = result.get("evaluations", [])
                
                # å°†è¯„ä¼°ç»“æœä¸åŸå§‹æ¡ç›®åˆå¹¶
                for eval_result in batch_evaluations:
                    sentence_id = eval_result["sentence_id"]
                    # æŸ¥æ‰¾å¯¹åº”çš„åŸå§‹æ¡ç›®
                    original_item = next((item for item in batch if item['sentence_id'] == sentence_id), None)
                    if original_item:
                        evaluation = {
                            'source_file': '',  # åœ¨è°ƒç”¨è€…å¤„è®¾ç½®
                            'index': original_item['index'],
                            'sentence_text': original_item['text'],
                            'assigned_category': original_item['category'],
                            'assigned_layer': original_item['layer'],
                            'original_reason': original_item['reason'],
                            'category_is_correct': eval_result['category_is_correct'],
                            'category_evaluation_reason': eval_result['category_evaluation_reason'],
                            'layer_is_correct': eval_result['layer_is_correct'],
                            'layer_evaluation_reason': eval_result['layer_evaluation_reason'],
                            'final_category': eval_result.get('final_category', ''),
                            'final_layer': eval_result.get('final_layer', ''),
                            'evaluation_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'evaluator': f'GT_Auto_{model}'
                        }
                        evaluations.append(evaluation)
        
        except Exception as e:
            print(f"  âŒ æ‰¹æ¬¡ {batch_idx+1}: å¤„ç†æ—¶å‡ºé”™: {e}")
            # ä¸ºå½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰æ¡ç›®åˆ›å»ºé”™è¯¯è®°å½•
            for item in batch:
                evaluation = {
                    'source_file': '',
                    'index': item['index'],
                    'sentence_text': item['text'],
                    'assigned_category': item['category'],
                    'assigned_layer': item['layer'],
                    'original_reason': item['reason'],
                    'category_is_correct': -1,
                    'category_evaluation_reason': f"Processing error: {str(e)}",
                    'layer_is_correct': -1,
                    'layer_evaluation_reason': f"Processing error: {str(e)}",
                    'final_category': '',
                    'final_layer': '',
                    'evaluation_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'evaluator': f'GT_Auto_{model}'
                }
                evaluations.append(evaluation)
    
    return evaluations

def _extract_json_from_content(content: str) -> str | None:
    """å¢å¼ºç‰ˆJSONæå–å‡½æ•° - ä»LLMè¿”å›çš„contentä¸­æå–JSONå­—ç¬¦ä¸²"""
    if not content or not isinstance(content, str):
        return None
    
    content = content.strip()
    
    # ç­–ç•¥1: ç›´æ¥å°è¯•è§£æï¼ˆæœ€ä¸¥æ ¼ï¼‰
    if content.startswith("{") and content.endswith("}"):
        try:
            json.loads(content)
            return content
        except:
            pass
    
    # ç­–ç•¥2: å»é™¤ä»£ç å—åŒ…è£…
    if "```" in content:
        # åŒ¹é… ```json ... ``` æˆ– ``` ... ```
        code_blocks = re.findall(r"```(?:json)?\s*(\{.*?\})\s*```", content, re.S | re.I)
        for block in code_blocks:
            try:
                json.loads(block)
                return block
            except:
                continue
    
    # ç­–ç•¥3: æŸ¥æ‰¾ Function Call æ ¼å¼
    # åŒ¹é… "Arguments: { ... }" æˆ– "Arguments: { ... }" ç­‰å˜ä½“
    function_patterns = [
        r"Arguments\s*:?\s*(\{.*\})",  # ä½¿ç”¨è´ªå©ªåŒ¹é…ï¼Œç¡®ä¿è·å–å®Œæ•´JSON
        r"Function\s*Call\s*:?\s*.*?Arguments\s*:?\s*(\{.*\})",  # ä½¿ç”¨è´ªå©ªåŒ¹é…
        r"function\s*:?\s*.*?arguments\s*:?\s*(\{.*\})",  # ä½¿ç”¨è´ªå©ªåŒ¹é…
    ]
    
    for pattern in function_patterns:
        matches = re.findall(pattern, content, re.S | re.I)
        for match in matches:
            try:
                json.loads(match)
                return match
            except:
                continue
    
    # ç­–ç•¥4: æŸ¥æ‰¾åŒ…å« "evaluations" é”®çš„JSONå¯¹è±¡
    # è¿™æ˜¯æœ€å…³é”®çš„ç­–ç•¥ï¼Œå› ä¸ºæˆ‘ä»¬è¦æ‰¾çš„å°±æ˜¯åŒ…å«evaluationsæ•°ç»„çš„JSON
    evaluation_patterns = [
        r"(\{[^{}]*\"evaluations\"[^{}]*\})",  # ç®€å•åŒ¹é…
        r"(\{[^{}]*\"evaluations\"[^{}]*\[[^\]]*\][^{}]*\})",  # åŒ…å«æ•°ç»„
    ]
    
    for pattern in evaluation_patterns:
        matches = re.findall(pattern, content, re.S)
        for match in matches:
            try:
                # å°è¯•æ‰©å±•åŒ¹é…åˆ°å®Œæ•´çš„JSONå¯¹è±¡
                # ä»matchå¼€å§‹ï¼Œå‘å‰åæ‰©å±•ï¼Œæ‰¾åˆ°å®Œæ•´çš„JSON
                start_pos = content.find(match)
                if start_pos != -1:
                    # å‘å‰æ‰¾ { çš„å¼€å§‹
                    brace_count = 0
                    start_idx = start_pos
                    for i in range(start_pos, -1, -1):
                        if content[i] == "}":
                            brace_count += 1
                        elif content[i] == "{":
                            brace_count -= 1
                            if brace_count == 0:
                                start_idx = i
                                break
                    
                    # å‘åæ‰¾ } çš„ç»“æŸ
                    brace_count = 0
                    end_idx = start_pos + len(match)
                    for i in range(start_pos + len(match), len(content)):
                        if content[i] == "{":
                            brace_count += 1
                        elif content[i] == "}":
                            brace_count -= 1
                            if brace_count == 0:
                                end_idx = i + 1
                                break
                    
                    full_json = content[start_idx:end_idx]
                    try:
                        json.loads(full_json)
                        return full_json
                    except:
                        pass
            except:
                continue
    
    # ç­–ç•¥5: æ™ºèƒ½æå–æœ€å¤–å±‚JSONå¯¹è±¡
    # ä½¿ç”¨æ›´æ™ºèƒ½çš„æ‹¬å·åŒ¹é…
    try:
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ª { çš„ä½ç½®
        start_pos = content.find("{")
        if start_pos == -1:
            return None
        
        # ä»ç¬¬ä¸€ä¸ª { å¼€å§‹ï¼Œè®¡ç®—æ‹¬å·åŒ¹é…
        brace_count = 0
        end_pos = -1
        
        for i in range(start_pos, len(content)):
            if content[i] == "{":
                brace_count += 1
            elif content[i] == "}":
                brace_count -= 1
                if brace_count == 0:
                    end_pos = i + 1
                    break
        
        if end_pos > start_pos:
            json_str = content[start_pos:end_pos]
            try:
                json.loads(json_str)
                return json_str
            except:
                pass
    except:
        pass
    
    # ç­–ç•¥6: æœ€åå°è¯• - æŸ¥æ‰¾ä»»ä½•çœ‹èµ·æ¥åƒJSONçš„å†…å®¹
    # åŒ¹é…åŒ…å«å¸¸è§é”®çš„JSONç‰‡æ®µ
    json_like_patterns = [
        r"(\{[^{}]*\"sentence_id\"[^{}]*\})",
        r"(\{[^{}]*\"category_is_correct\"[^{}]*\})",
        r"(\{[^{}]*\"layer_is_correct\"[^{}]*\})",
    ]
    
    for pattern in json_like_patterns:
        matches = re.findall(pattern, content, re.S)
        for match in matches:
            try:
                # å°è¯•æ‰©å±•ä¸ºå®Œæ•´JSON
                start_pos = content.find(match)
                if start_pos != -1:
                    # ç®€å•çš„æ‹¬å·åŒ¹é…æ‰©å±•
                    brace_count = 0
                    start_idx = start_pos
                    for i in range(start_pos, -1, -1):
                        if content[i] == "}":
                            brace_count += 1
                        elif content[i] == "{":
                            brace_count -= 1
                            if brace_count == 0:
                                start_idx = i
                                break
                    
                    brace_count = 0
                    end_idx = start_pos + len(match)
                    for i in range(start_pos + len(match), len(content)):
                        if content[i] == "{":
                            brace_count += 1
                        elif content[i] == "}":
                            brace_count -= 1
                            if brace_count == 0:
                                end_idx = i + 1
                                break
                    
                    full_json = content[start_idx:end_idx]
                    try:
                        json.loads(full_json)
                        return full_json
                    except:
                        pass
            except:
                continue
    
    return None

def evaluate_with_ollama(items: List[Dict], batch_size: int, model: str, ollama_client: OllamaClient, temperature: float = 0.1, enable_few_shot: bool = True) -> List[Dict]:
    """ä½¿ç”¨Ollamaè¿›è¡ŒHFACSåˆ†ç±»è¯„ä¼°"""
    evaluations = []
    
    for batch_idx, batch in enumerate(tqdm(list(chunk(items, batch_size)), desc="Ollama-evaluating")):
        # æ„å»ºè¯„ä¼°å†…å®¹
        evaluation_content = "Evaluate these HFACS classifications:\n\n"
        for item in batch:
            evaluation_content += f"ID: {item['sentence_id']}\n"
            evaluation_content += f"Text: \"{item['text']}\"\n"
            evaluation_content += f"Assigned Category: {item['category']}\n"
            evaluation_content += f"Assigned Layer: {item['layer']}\n"
            evaluation_content += f"Original Reason: \"{item['reason']}\"\n\n"
        
        # æ„å»ºæ¶ˆæ¯
        messages = [{"role": "system", "content": EVALUATION_SYSTEM_PROMPT}]
        
        # æ ¹æ®å¼€å…³å†³å®šæ˜¯å¦æ·»åŠ few-shotç¤ºä¾‹
        if enable_few_shot:
            messages.extend(EVALUATION_FEW_SHOT)
        
        # æ·»åŠ å½“å‰è¯„ä¼°ä»»åŠ¡
        messages.append({"role": "user", "content": evaluation_content})
        
        # ä¸ºæœ¬åœ°LLMæ·»åŠ æ ¼å¼æŒ‡å¯¼
        format_instruction = {
            "role": "user",
            "content": """Please respond with a function call format like this:

**Function Call:**
Function: evaluate_hfacs_classification
Arguments: {"evaluations": [your_evaluation_results_array]}

Where each evaluation result should have:
- sentence_id: the ID from input
- category_is_correct: 1 (correct), 0 (incorrect), or -1 (skip)
- category_evaluation_reason: brief reason for category evaluation
- layer_is_correct: 1 (correct), 0 (incorrect), or -1 (skip)  
- layer_evaluation_reason: brief reason for layer evaluation
- final_category: final HFACS category (if correct=1, use original; if 0, select correct one from 18 categories; if -1, use empty string)
- final_layer: final HFACS layer (if correct=1, use original; if 0, select correct one from 4 layers; if -1, use empty string)"""
        }
        messages.append(format_instruction)
        
        try:
            # âš ï¸ ä¸ä½¿ç”¨ format=jsonï¼Œä»¥ä¿ç•™ function call æ ¼å¼
            response = ollama_client.chat_completion(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=4000,
                force_json=False
            )

            content: str = response.get("message", {}).get("content", "")

            # è§£ææ­¥éª¤ â‘ ï¼šç›´æ¥å°è¯• JSON.loads
            evaluations_data = []
            if content.strip().startswith("{"):
                try:
                    evaluations_data = json.loads(content).get("evaluations", [])
                except Exception:
                    evaluations_data = []

            # è§£ææ­¥éª¤ â‘¡ï¼šè¾…åŠ©å‡½æ•°æå–JSON
            if not evaluations_data:
                json_block = _extract_json_from_content(content)
                if json_block:
                    try:
                        evaluations_data = json.loads(json_block).get("evaluations", [])
                    except Exception:
                        evaluations_data = []
            
            if evaluations_data and isinstance(evaluations_data, list):
                # å°†è¯„ä¼°ç»“æœä¸åŸå§‹æ¡ç›®åˆå¹¶
                for eval_result in evaluations_data:
                    if not isinstance(eval_result, dict):
                        continue
                            
                    sentence_id = eval_result.get("sentence_id", "")
                    # æŸ¥æ‰¾å¯¹åº”çš„åŸå§‹æ¡ç›®
                    original_item = next((item for item in batch if item['sentence_id'] == sentence_id), None)
                    if original_item:
                        evaluation = {
                            'source_file': '',
                            'index': original_item['index'],
                            'sentence_text': original_item['text'],
                            'assigned_category': original_item['category'],
                            'assigned_layer': original_item['layer'],
                            'original_reason': original_item['reason'],
                            'category_is_correct': eval_result.get('category_is_correct', -1),
                            'category_evaluation_reason': eval_result.get('category_evaluation_reason', ''),
                            'layer_is_correct': eval_result.get('layer_is_correct', -1),
                            'layer_evaluation_reason': eval_result.get('layer_evaluation_reason', ''),
                            'final_category': eval_result.get('final_category', ''),
                            'final_layer': eval_result.get('final_layer', ''),
                            'evaluation_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'evaluator': f'GT_Auto_{model}'
                        }
                        evaluations.append(evaluation)
            else:
                raise ValueError("æ— æ³•è§£ææœ‰æ•ˆçš„evaluationsæ•°ç»„")
                    
        except Exception as e:
            print(f"  âŒ æ‰¹æ¬¡ {batch_idx+1}: å¤„ç†æ—¶å‡ºé”™: {e}")
            
            # å¢å¼ºè°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºåŸå§‹å“åº”çš„å‰200å­—ç¬¦
            if 'content' in locals():
                debug_content = content[:200] + "..." if len(content) > 200 else content
                print(f"  ğŸ” åŸå§‹å“åº”é¢„è§ˆ: {debug_content}")
            
            # ä¸ºå½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰æ¡ç›®åˆ›å»ºé”™è¯¯è®°å½•
            for item in batch:
                evaluation = {
                    'source_file': '',
                    'index': item['index'],
                    'sentence_text': item['text'],
                    'assigned_category': item['category'],
                    'assigned_layer': item['layer'],
                    'original_reason': item['reason'],
                    'category_is_correct': -1,
                    'category_evaluation_reason': f"Processing error: {str(e)}",
                    'layer_is_correct': -1,
                    'layer_evaluation_reason': f"Processing error: {str(e)}",
                    'final_category': '',
                    'final_layer': '',
                    'evaluation_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'evaluator': f'GT_Auto_{model}'
                }
                evaluations.append(evaluation)
    
    return evaluations

def find_json_files(input_path: str, file_pattern: str = "*_classified_results_*.json") -> List[str]:
    """æŸ¥æ‰¾è¾“å…¥è·¯å¾„ä¸­ç¬¦åˆæ¨¡å¼çš„JSONæ–‡ä»¶"""
    if os.path.isfile(input_path):
        return [input_path]
    elif os.path.isdir(input_path):
        json_files = []
        patterns = [file_pattern, f"**/{file_pattern}"]
        for pattern in patterns:
            json_files.extend(glob.glob(os.path.join(input_path, pattern), recursive=True))
        
        if not json_files and file_pattern != "*.json":
            print(f"âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¨¡å¼ '{file_pattern}' çš„æ–‡ä»¶ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶...")
            fallback_patterns = ['*.json', '**/*.json']
            for pattern in fallback_patterns:
                json_files.extend(glob.glob(os.path.join(input_path, pattern), recursive=True))
        
        return sorted(json_files)
    else:
        raise ValueError(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")

def process_single_json(json_file: str, batch_size: int, model: str, client, temperature: float, output_dir: str = None, enable_few_shot: bool = True, model_type: str = "openai") -> Dict[str, Any]:
    """å¤„ç†å•ä¸ªJSONæ–‡ä»¶è¿›è¡Œè‡ªåŠ¨è¯„ä¼°"""
    try:
        # åŠ è½½åˆ†ç±»ç»“æœ
        classification_data = load_classification_results(json_file)
        items = extract_classification_items(classification_data)
        
        if not items:
            print(f"âš ï¸ æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åˆ†ç±»æ¡ç›®: {json_file}")
            return {
                'input_file': json_file,
                'error': 'æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åˆ†ç±»æ¡ç›®',
                'model': model,
                'model_type': model_type,
                'enable_few_shot': enable_few_shot
            }
        
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        model_name = model.replace(':', '_').replace('/', '_')
        base_name = os.path.basename(json_file)
        if base_name.endswith('.json'):
            base_name = base_name.replace('.json', '')
        
        output_filename = f"{base_name}_gt_evaluation_{model_name}.csv"
        
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            output_file = os.path.join(output_dir, output_filename)
        else:
            input_dir = os.path.dirname(json_file)
            output_file = os.path.join(input_dir, output_filename)
        
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
        if os.path.exists(output_file):
            print(f"â­ï¸ è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶: {json_file}")
            return {
                'input_file': json_file,
                'output_file': output_file,
                'skipped': True,
                'reason': 'æ–‡ä»¶å·²å­˜åœ¨',
                'model': model,
                'model_type': model_type,
                'enable_few_shot': enable_few_shot
            }
        
        print(f"ğŸ¤– ä½¿ç”¨ {model} è¯„ä¼°ä¸­...")
        
        # è¿›è¡Œè¯„ä¼°
        if model_type == "openai":
            evaluations = evaluate_with_openai(items, batch_size, model, client, temperature, enable_few_shot)
        elif model_type == "ollama":
            evaluations = evaluate_with_ollama(items, batch_size, model, client, temperature, enable_few_shot)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        # è®¾ç½®æºæ–‡ä»¶ä¿¡æ¯
        source_file = os.path.basename(json_file)
        for eval_item in evaluations:
            eval_item['source_file'] = source_file
        
        # ä¿å­˜ç»“æœä¸ºCSV
        df = pd.DataFrame(evaluations)
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        generate_summary_report(evaluations, output_file, model)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_evaluations = len(evaluations)
        category_correct = len([e for e in evaluations if e.get('category_is_correct') == 1])
        category_incorrect = len([e for e in evaluations if e.get('category_is_correct') == 0])
        layer_correct = len([e for e in evaluations if e.get('layer_is_correct') == 1])
        layer_incorrect = len([e for e in evaluations if e.get('layer_is_correct') == 0])
        
        print(f"âœ… è¯„ä¼°å®Œæˆ: {total_evaluations} ä¸ªæ¡ç›®")
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š ç±»åˆ«è¯„ä¼°: {category_correct}æ­£ç¡®, {category_incorrect}é”™è¯¯")
        print(f"ğŸ“Š å±‚çº§è¯„ä¼°: {layer_correct}æ­£ç¡®, {layer_incorrect}é”™è¯¯")
        
        return {
            'input_file': json_file,
            'output_file': output_file,
            'total_evaluations': total_evaluations,
            'category_correct': category_correct,
            'category_incorrect': category_incorrect,
            'layer_correct': layer_correct,
            'layer_incorrect': layer_incorrect,
            'model': model,
            'model_type': model_type,
            'batch_size': batch_size,
            'enable_few_shot': enable_few_shot
        }
        
    except Exception as e:
        error_msg = f"å¤„ç† {json_file} æ—¶å‡ºé”™: {str(e)}"
        print(f"âŒ {error_msg}")
        return {
            'input_file': json_file,
            'error': error_msg,
            'model': model,
            'model_type': model_type,
            'enable_few_shot': enable_few_shot
        }

def generate_summary_report(evaluations: List[Dict], csv_path: str, model: str):
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    if not evaluations:
        return
    
    # ç»Ÿè®¡ç»“æœ
    total = len(evaluations)
    category_correct = len([e for e in evaluations if e.get('category_is_correct') == 1])
    category_incorrect = len([e for e in evaluations if e.get('category_is_correct') == 0])
    category_skipped = len([e for e in evaluations if e.get('category_is_correct') == -1])
    
    layer_correct = len([e for e in evaluations if e.get('layer_is_correct') == 1])
    layer_incorrect = len([e for e in evaluations if e.get('layer_is_correct') == 0])
    layer_skipped = len([e for e in evaluations if e.get('layer_is_correct') == -1])
    
    # è®¡ç®—æ­£ç¡®ç‡
    category_valid = total - category_skipped
    layer_valid = total - layer_skipped
    category_accuracy = (category_correct / category_valid * 100) if category_valid > 0 else 0
    layer_accuracy = (layer_correct / layer_valid * 100) if layer_valid > 0 else 0
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    summary_path = csv_path.replace('.csv', '_summary.txt')
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(f"GT_Run_Auto è‡ªåŠ¨è¯„ä¼°æ±‡æ€»æŠ¥å‘Š\n")
        f.write(f"=" * 50 + "\n\n")
        f.write(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"è¯„ä¼°æ¨¡å‹: {model}\n")
        f.write(f"æºæ–‡ä»¶: {evaluations[0].get('source_file', 'Unknown')}\n\n")
        
        f.write(f"æ€»ä½“ç»Ÿè®¡:\n")
        f.write(f"  æ€»æ¡ç›®æ•°: {total}\n\n")
        
        f.write(f"18ç±»åˆ«è¯„ä¼°:\n")
        f.write(f"  æœ‰æ•ˆè¯„ä¼°: {category_valid}\n")
        f.write(f"  æ­£ç¡®: {category_correct}\n")
        f.write(f"  é”™è¯¯: {category_incorrect}\n")
        f.write(f"  è·³è¿‡: {category_skipped}\n")
        f.write(f"  æ­£ç¡®ç‡: {category_accuracy:.1f}%\n\n")
        
        f.write(f"4å±‚çº§è¯„ä¼°:\n")
        f.write(f"  æœ‰æ•ˆè¯„ä¼°: {layer_valid}\n")
        f.write(f"  æ­£ç¡®: {layer_correct}\n")
        f.write(f"  é”™è¯¯: {layer_incorrect}\n")
        f.write(f"  è·³è¿‡: {layer_skipped}\n")
        f.write(f"  æ­£ç¡®ç‡: {layer_accuracy:.1f}%\n")

def create_batch_summary(results: List[Dict[str, Any]], output_dir: str) -> Dict[str, Any]:
    """åˆ›å»ºæ‰¹é‡å¤„ç†æ±‡æ€»æŠ¥å‘Š"""
    successful = [r for r in results if 'output_file' in r and 'error' not in r and not r.get('skipped', False)]
    skipped = [r for r in results if r.get('skipped', False)]
    errors = [r for r in results if 'error' in r]
    
    summary = {
        "timestamp": datetime.now().isoformat(),
        "total_files": len(results),
        "successful": len(successful),
        "skipped": len(skipped),
        "errors": len(errors),
        "results": results
    }
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    summary_file = os.path.join(output_dir, "batch_summary_gt_auto.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“‹ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_file}")
    return summary

# æ·»åŠ Ollamaæ¨¡å‹åˆ—è¡¨è·å–å‡½æ•°
def get_installed_models(ollama_client: OllamaClient) -> List[str]:
    """è·å–å·²å®‰è£…çš„æ¨¡å‹åˆ—è¡¨"""
    try:
        url = f"{ollama_client.base_url}/api/tags"
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        models = []
        for model in data.get("models", []):
            model_name = model.get("name", "")
            if model_name:
                models.append(model_name)
        
        return sorted(models)
        
    except Exception as e:
        print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return []

def main():
    ap = argparse.ArgumentParser(description="GT_Run_Auto - HFACSåˆ†ç±»è‡ªåŠ¨åŒ–è¯„ä¼°å™¨")
    ap.add_argument("input", nargs='?')
    ap.add_argument("--batch", type=int, default=10)
    ap.add_argument("--model", default="gpt-4o-mini")
    ap.add_argument("--model-type", choices=["openai", "ollama"], default="openai")
    ap.add_argument("--ollama-url", default="http://localhost:11434")
    ap.add_argument("--output-dir")
    ap.add_argument("--temperature", type=float, default=0.1)
    ap.add_argument("--file-pattern", default="*_classified_results_*.json")
    ap.add_argument("--list-models", action="store_true", help="åˆ—å‡ºOllamaå·²å®‰è£…æ¨¡å‹")
    fewshot_group = ap.add_mutually_exclusive_group()
    fewshot_group.add_argument("--enable-few-shot", dest="enable_few_shot", action="store_true", help="ä½¿ç”¨few-shotç¤ºä¾‹ (é»˜è®¤)")
    fewshot_group.add_argument("--disable-few-shot", dest="enable_few_shot", action="store_false", help="ä¸ä½¿ç”¨few-shotç¤ºä¾‹")
    ap.set_defaults(enable_few_shot=True)
    args = ap.parse_args()

    # å¦‚æœè¯·æ±‚åˆ—å‡ºæ¨¡å‹
    if args.list_models:
        if args.model_type != "ollama":
            print("--list-models ä»…æ”¯æŒæœ¬åœ°Ollamaæ¨¡å¼ (è¯·åŠ  --model-type ollama)")
            return
        client = OllamaClient(args.ollama_url)
        models = get_installed_models(client)
        if models:
            print("ğŸ“‹ å·²å®‰è£…çš„Ollamaæ¨¡å‹:")
            for i, model in enumerate(models, 1):
                print(f"  {i}. {model}")
        else:
            print("âŒ æœªæ‰¾åˆ°å·²å®‰è£…çš„Ollamaæ¨¡å‹")
        return

    # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®å®¢æˆ·ç«¯
    if args.model_type == "openai":
        # è®¾ç½®OpenAI API Key
        if not os.getenv("OPENAI_API_KEY"):
            os.environ["OPENAI_API_KEY"] = "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"
        
        client = OpenAI()
        print(f"ğŸ¤– ä½¿ç”¨OpenAIæ¨¡å‹: {args.model}")
    else:
        client = OllamaClient(args.ollama_url)
        print(f"ğŸ” æµ‹è¯•Ollamaè¿æ¥: {args.ollama_url}")
        if not test_ollama_connection(client, args.model):
            sys.exit("âŒ æ— æ³•è¿æ¥åˆ°Ollamaæˆ–æ¨¡å‹ä¸å¯ç”¨")
        print(f"ğŸ¤– ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {args.model}")

    # æ£€æŸ¥æ˜¯å¦æä¾›äº†è¾“å…¥æ–‡ä»¶
    if not args.input:
        print("âŒ è¯·æä¾›è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„")
        print("ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
        return

    # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
    json_files = find_json_files(args.input, args.file_pattern)
    
    if not json_files:
        sys.exit(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„JSONæ–‡ä»¶: {args.input}")
    
    print(f"ğŸ” æ‰¾åˆ° {len(json_files)} ä¸ª JSON æ–‡ä»¶")
    for i, json_file in enumerate(json_files, 1):
        print(f"  {i}. {json_file}")
    
    # ç¡®è®¤æ˜¯å¦ç»§ç»­
    if len(json_files) > 1:
        response = input(f"\næ˜¯å¦ç»§ç»­å¤„ç†è¿™ {len(json_files)} ä¸ªæ–‡ä»¶? (y/N): ")
        if response.lower() not in ['y', 'yes', 'æ˜¯']:
            print("å–æ¶ˆå¤„ç†")
            return
    
    # æ‰¹é‡å¤„ç†
    results = []
    for i, json_file in enumerate(json_files, 1):
        print(f"\nğŸ“ å¤„ç†æ–‡ä»¶ {i}/{len(json_files)}: {os.path.basename(json_file)}")
        result = process_single_json(json_file, args.batch, args.model, client, args.temperature, args.output_dir, args.enable_few_shot, args.model_type)
        results.append(result)
    
    # åˆ›å»ºæ±‡æ€»æŠ¥å‘Š
    if len(json_files) > 1:
        output_dir = args.output_dir or "batch_results_gt_auto"
        os.makedirs(output_dir, exist_ok=True)
        summary = create_batch_summary(results, output_dir)
        
        print(f"\nğŸ‰ æ‰¹é‡è¯„ä¼°å®Œæˆ!")
        print(f"âœ… æˆåŠŸ: {summary['successful']}")
        print(f"â­ï¸  è·³è¿‡: {summary['skipped']}")
        print(f"âŒ é”™è¯¯: {summary['errors']}")
    else:
        print(f"\nğŸ‰ è¯„ä¼°å®Œæˆ!")

if __name__ == "__main__":
    main() 